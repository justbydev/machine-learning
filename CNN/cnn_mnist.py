# -*- coding: utf-8 -*-
"""CNN_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L3lf7Kf-ICXvQviaQI97CP9eJxnZhEPH

Import modules
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Dropout
from keras.layers import Conv2D, MaxPooling2D
from keras.datasets import mnist
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""Tuning parameters"""

epoch=10
learning_rate=0.01
batch_size=128
num_classes=10

"""Load Mnist dataset"""

(train_images, train_labels), (test_images, test_labels)=mnist.load_data()
#keras의 mnist 데이터는 traning set과 test set이 따로 잇는데 image와 label이 따로 있지는 않다. 따라서, load 할때, 따로 저장하기 위한 방법

"""Reshaping dataset"""

print(train_images.shape)
print(train_labels.shape)
print(test_images.shape)
print(test_labels.shape)

train_images=train_images.reshape(train_images.shape[0], train_images.shape[1], 
                     train_images.shape[2], 1)
test_images=test_images.reshape(test_images.shape[0], test_images.shape[1], 
                                test_images.shape[2], 1)
train_images=train_images.astype('float32')
test_images=test_images.astype('float32')
train_images/=255
test_images/=255
#기본적으로 mnist dataset은 grayscale이다. grayscale image는 8bit로 이루어져 있기 때문에 0~255까지의 수로 이루어져 있다.
#따라서, 이와 같은 수를 0~1로 바꾸어 주기 위해서 float으로 type 변환 시키고 255로 나누게 된다.

print(train_images.shape)
print(test_images.shape)
print(train_images.shape[0])
print(test_images.shape[0])

"""Applying One hot encoding for the data"""

#one hot encoding 이란 것은 각각의 데이터에 대해서 indexing을 시켜주기 위한 것이다.
#즉, 0~9까지의 손글씨 dataset에 각각 0부터 9까지 indexing을 부여하는 방법이다.
#이것은 keras에서 to_categorical()로 잘 표현되어 있다.
train_labels=keras.utils.to_categorical(train_labels, num_classes)#num_classes=10
test_labels=keras.utils.to_categorical(test_labels, num_classes)

"""Creating the DNN model for CNN operation"""

model=Sequential() #layer 쌓기 위한 model 준비

"""Adding layers to the model"""

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',
                 padding='valid'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
#여기까지가 convolution layer 단계
#convolution으로 feature을 뽑아낼 때 뒤로 갈수록 filter수를 증가시켜 더 세세하게 뽑도록 설정
model.add(MaxPooling2D(pool_size=(2, 2)))
#여기가 pooling layer 단계
model.add(Dropout(0.25))
#overfitting 방지를 위한 dropout
model.add(Flatten())
#FC Layer를 실행하기 전에 2차원의 feature map을 1차원으로 만드는 Flatten
model.add(Dense(num_classes, activation='softmax'))
#결과적으로 출력은 10개, 확률로 결과를 얻기 위해 softmax를 사용

"""Optimizer 설정"""

optimizer=keras.optimizers.SGD(lr=learning_rate)
#sophisticated gradient descent 사용

"""Compile the model"""

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=optimizer,
              metrics=['accuracy'])
#cost function을 quaradic mse 대신에 learning 속도가 개선된 cross_entropy 사용

"""Training the model"""

model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epoch,
          verbose=1,
          validation_data=(test_images, test_labels))

"""Evaluating the model"""

score=model.evaluate(test_images, test_labels, verbose=0)
print('Test loss:', score[0])
print('Test accuracy', score[1])

"""Prediction the model"""

test_pred=model.predict(test_images)

test_result=confusion_matrix(test_labels.argmax(axis=1), test_pred.argmax(axis=1))
sns.heatmap(pd.DataFrame(test_result, range(10), range(10)), annot=True, fmt='g')