1) 토큰화
주어진 코퍼스에서 토큰이라 불리는 단위로 나누는 작업
->단어 토큰화
-->from nltk.tokenize import word_tokenize
-->from nltk.tokenize import WordPunctTokenizer
-->from tensorflow.keras.preprocessing.text import text_to_word_sequence
-->구두점, 특수문자, 줄임말, 단어 내 띄어쓰기 주의
->문장 코튼화
-->from nltk.tokenize import sent_tokenize
-->한글: import kss, kss.split_sentences()

->한국어의 토큰화의 어려움-->한국어는 결국 형태소 기준이며 띄어쓰기가 잘 지켜지지 않는다

한국어 토큰화->konlpy.tag 사용
ex) Okt, Mecab, Komoran, Hannanum, Kkma 등 다양하며 선택해서 사용

2) 정제 & 정규화
토큰화 작업 전, 후에는 정제, 정규화 과정을 하게 됩니다.
정제: 갖고 있는 코퍼스로부터 노이즈 데이터 제거
정규화: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦
->규칙, 대소문자 통합, 불필요 단어 제거(등장 빈도, 길이 등), 정규 표현식 사용
->이 부분은 활용 분야에 맞게 정제 및 정규화 필요

3) 어간 추출(stemming) & 표제어 추출(lemmatization)
->정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법
-->from nltk.stem import WordNetLemmatizer->표제어 추출
-->from nltk.stem import PorterStemmer->어간 추출
-->from nltk.stem import LancasterStemmer->어간 추출
->위의 두 가지는 문장이 있으면 word_tokenize로 단어 단위로 나눈 후(토큰화) 각 단어의 어간을 뽑아낸다
한국어의 어간 추출의 경우 komoran을 사용하여 형태소 분석을 하면 쉽게 가능

stemming의 경우 flies가 주어지면 단순히 어간(어근)을 찾아주지만
lemmatization의 경우 flies가 동사 날다와 명사 파리 중 어떤 뜻으로 쓰였는지까지 결정
->그래서 사실 한글의 경우 형태소 분석을 먼저 해준 후, 어간, 표제어 추출을 해주게 된다.

4) 불용어 제거
정제와 비슷한 개념으로 코퍼스로부터 만들어진 토큰 중에서 큰 의미가 없는 단어 토큰을 제거하는 작업
->자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들
->영어의 경우
from nltk.corpus import stopwords의 stopwords.words('english')가 존재
->한국어의 경우
직접 불용어 리스트를 만들고, word_tokenize를 통해서 토큰화를 한 후, 직접 만든 불용어 리스트를 통해
불용어를 제거한다
혹은, 형태소 분석을 통해서 원하는 형태소를 제거

5) 정규표현식 처리
->import re 사용
->정규표현식을 사용한 단어 토큰화
-->from nltk.tokenize import RegexpTokenizer
ex) tokenizer=RegexpTokenizer("[\w]+")->문자 또는 숫자가 1개 이상인 경우 인식, 구두점 제외

6) 정수 인코딩
->단어에 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법
-->from nltk.tokenize import sent_tokenize를 통해 문장 토큰화
-->from nltk.tokenize import word_tokenize를 통해 단어 코튼화
-->stopwords 혹은 직접 만든 불용어 리스트 통해 불용어 제거 및 정제
-->from collections import Counter/from nltk import FreqDist 를 사용하여 각 단어의 빈도수 측정
-->from tensorflow.keras.preprocessing.text import Tokenizer도 사용 가능
-->tokenizer=Tokenizer()
-->tokenizer.fit_on_texts(원하는 문장 집합)->빈도수 기준 단어 집합 생성
-->tokenizer.texts_to_sequences(원하는 문장 집합)->정수 인코딩한 집합 생성
-->tokenizer=Tokenizer(num_words=vocab_size+1)->vocab_size=5라면 상위 5개 단어만 사용
-->빈도수 측정 후 정렬
-->기준 정하여 빈도수 낮은 것은 제거 후 Out of Vocabulary(단어집합에 없는 단어) 표시
-->정수 인코딩 실행
위와 같은 단계로 실행

7) 패딩
->nlp를 실행하다보면 문장 길이가 서로 다른데 기계는 길이가 전부 동일한 문장들에 대해서 하나의 행렬로 보고
 한꺼번에 묶어서 처리할 수 있기에 패딩 처리함
->가장 길이가 긴(단어수로) 문장을 기준으로 하고 정수 인코딩을 한 후, 
  0으로 채우거나 가장 큰 인코딩 보다 하나 큰 숫자로 패딩을 실행

-->from tensorflow.keras.preprocessing.sequence import pad_sequences
-->정수 인코딩 후 패딩 진행
-->pad_sequences(정수 인코딩 결과)
-->pad_sequences(정수 인코딩 결과 padding='post', maxlen=5)->0을 뒤에, 최대 길이 지정 가능

8) one-hot encoding
->서로 다른 단어들을 벡터로써 표현하는 것으로 단수, 복수형 상관없이 다 다른 것으로 취급
->정수 인코딩을 통해서 부여한 정수를 기반으로 진행
-->정수 인코딩을 통해서 각 단어에 정수 부여
-->from tensorflow.keras.utils import to_categorical 사용
-->예를 들면 '점심 먹으러 식당에 간다' 라고 한다면 {'점심':1, '먹으로':2, '식당에':3, '간다':4} 라고 한다면
    점심을 [0, 1, 0, 0, 0], 먹으로를 [0, 0, 1, 0, 0], 식당에를 [0, 0, 0, 1, 0], 간다를 [0, 0, 0, 0, 1]로 바꾸는 것이 원-핫 인코딩

9) 데이터 분리
->train dataset과 test dataset을 분리
-->from sklearn.model_selection import train_test_split 사용

from ckonlpy.tag import Twitter
twitter=Twitter()
twitter.add_dictionary('은욱이', 'Noun')
이렇게 형태소 분석기에 단어, 품사 같은 형식으로 추가하면 이것 역시 제대로 인식됨
->이렇게, customized KoNLPy를 사용하게 되면 인식률을 높일 수 있다