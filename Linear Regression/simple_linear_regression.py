# -*- coding: utf-8 -*-
"""simple_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXUY6xbN50a5uA4LjulRdwWaSP2TZ0--

Import modules
"""

import numpy as np
import matplotlib.pyplot as plt

"""Setting Data"""

data = np.array([[100, 20], #출처:우아한 형제들 기술 블로그, [배달거리, 배달시간]
		[150, 24], 
		[300, 36], 
		[400, 47], 
		[130, 22], 
		[240, 32],
		[350, 47], 
		[200, 42], 
		[100, 21], 
		[110, 21], 
		[190, 30], 
		[120, 25], 
		[130, 18], 
		[270, 38], 
		[255, 28]])
x=data[:, 0].reshape((data.shape[0], 1))
y=data[:, 1].reshape((data.shape[0], 1))
fig, ax=plt.subplots()
for i in range(data.shape[0]):
  ax.plot(x[i][0], y[i][0], marker='o', color='blue')

"""Setting Hyperparameter"""

learning_rate=0.00001
epoch=300000
weight=np.random.rand()
bias=np.random.randint(5, 20)
print(weight, bias)

"""Make Cost function(MSE)"""

def error_function(W, b):
  y_pred=x*W+b
  return np.sum((y_pred-y)**2)/len(y)

"""Predict"""

def predict():
  return weight*x+bias

"""Derivative of Error(using numerical derivative)"""

def numerical_derivative(f, W, b):
  h=1e-4
  grad=np.zeros((1, 2))
  w_fx1=error_function(float(W)+h, b)
  w_fx2=error_function(float(W)-h, b)
  grad[0, 0]=(w_fx1-w_fx2)/(2*h)
  b_fx1=error_function(W, float(b)+h)
  b_fx2=error_function(W, float(b)-h)
  grad[0, 1]=(b_fx1-b_fx2)/(2*h)
  return grad

"""Training"""

for i in range(epoch):
  grad=numerical_derivative(error_function, weight, bias)
  weight=weight-learning_rate*grad[0, 0]
  bias=bias-learning_rate*grad[0, 1]
  if i%15000==0:
    print('Epoch=', i, ' error_value=', error_function(weight, bias), "W=", weight, "b=", bias)