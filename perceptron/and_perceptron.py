# -*- coding: utf-8 -*-
"""AND_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ThPz_uGkpDjpaU1XUTSUH00J2QJ-sQo

Import modules
"""

import numpy as np
import matplotlib.pyplot as plt

"""make sigmoid function as activation function"""

def sigmoid(x):
  return 1/(1+np.exp(-x))

def derivative_sigmoid(x):
  return x*(1-x)

"""make Data"""

#input
x=np.array([[0, 0],
            [0, 1],
            [1, 0],
            [1, 1]])
#output
y=np.array([[0], [0], [0], [1]])
fig, ax=plt.subplots()
for i in range(4):
  if y[i][0]==0:
    ax.plot(x[i][0], x[i][1], marker='o', color='blue')
  else:
    ax.plot(x[i][0], x[i][1], marker='o', color='red')

"""initialize hyper_parameter"""

learning_rate=0.1
epoch=15000

weight=np.random.uniform(size=(2, 1))
bias=np.random.uniform(size=(1, 1))

initial_y=sigmoid(x.dot(weight)+bias)

"""Train"""

for i in range(epoch):
  y_pred=sigmoid(x.dot(weight)+bias)
  Error=((y-y_pred)**2)/2

  if i%500==0:
    print('Epoch', i, ':', Error.sum())
  gradient_y_pred=(y_pred-y)*(y_pred*(1-y_pred))
  gradient_weight=x.T.dot(gradient_y_pred)
  gradient_bias=np.sum(gradient_y_pred, axis=0, keepdims=True)

  weight=weight-learning_rate*gradient_weight
  bias=bias-learning_rate*gradient_bias

"""Result"""

print('Input')
print(x)
print('Label')
print(y)
print('Before deep learning')
print(initial_y)
print('After deep learning Output')
print(y_pred)